import os
import sys
import json
import asyncio
import random
import string
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool
from langchain_openai import ChatOpenAI
import redis
from celery import Celery
import logging
import pandas as pd

# Add config path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from config.environment import config
import numpy as np
from faker import Faker
import pytest
from playwright.async_api import async_playwright
import subprocess
import git
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RegressionTestingTool(BaseTool):
    name: str = "Regression Testing Suite"
    description: str = "Executes comprehensive regression tests with automated root cause detection"
    
    def _run(self, test_suite: Dict[str, Any], environment: str = "staging") -> Dict[str, Any]:
        """Execute regression test suite with root cause analysis"""
        execution_result = {
            "test_suite": test_suite.get("name", "unknown"),
            "environment": environment,
            "execution_time": None,
            "results": {
                "total_tests": 0,
                "passed": 0,
                "failed": 0,
                "skipped": 0
            },
            "failed_tests": [],
            "root_cause_analysis": None,
            "regression_detected": False
        }
        
        start_time = datetime.now()
        
        # Execute test cases
        test_cases = test_suite.get("test_cases", [])
        execution_result["results"]["total_tests"] = len(test_cases)
        
        for test_case in test_cases:
            test_result = self._execute_single_test(test_case, environment)
            
            if test_result["status"] == "passed":
                execution_result["results"]["passed"] += 1
            elif test_result["status"] == "failed":
                execution_result["results"]["failed"] += 1
                execution_result["failed_tests"].append({
                    "test_id": test_case["id"],
                    "test_name": test_case["name"],
                    "error_message": test_result["error"],
                    "stack_trace": test_result["stack_trace"]
                })
            else:
                execution_result["results"]["skipped"] += 1
        
        # Calculate execution time
        end_time = datetime.now()
        execution_result["execution_time"] = (end_time - start_time).total_seconds()
        
        # Perform root cause analysis for failures
        if execution_result["failed_tests"]:
            execution_result["root_cause_analysis"] = self._analyze_root_causes(
                execution_result["failed_tests"], test_suite
            )
            execution_result["regression_detected"] = self._detect_regression(
                execution_result["failed_tests"], test_suite
            )
        
        return execution_result
    
    def _execute_single_test(self, test_case: Dict[str, Any], environment: str) -> Dict[str, Any]:
        """Execute a single test case"""
        # Simulate test execution
        test_type = test_case.get("type", "functional")
        
        # Simulate different failure rates based on test type
        failure_rates = {
            "functional": 0.05,
            "integration": 0.10,
            "performance": 0.15,
            "security": 0.08
        }
        
        failure_rate = failure_rates.get(test_type, 0.05)
        
        if random.random() < failure_rate:
            return {
                "status": "failed",
                "error": f"Simulated failure in {test_case['name']}",
                "stack_trace": f"Traceback: Failed at line {random.randint(1, 100)}"
            }
        else:
            return {"status": "passed"}
    
    def _analyze_root_causes(self, failed_tests: List[Dict], test_suite: Dict) -> Dict[str, Any]:
        """Perform automated root cause analysis for failed tests"""
        # Cluster failures by similarity
        error_messages = [test["error_message"] for test in failed_tests]
        
        # Simple clustering based on error patterns
        root_causes = {
            "categories": [],
            "most_common_cause": None,
            "confidence_score": 0.0,
            "recommended_actions": []
        }
        
        # Analyze error patterns
        error_patterns = {
            "authentication": ["auth", "login", "token", "session"],
            "api_integration": ["api", "endpoint", "response", "timeout"],
            "ui_elements": ["element", "selector", "click", "display"],
            "data_validation": ["validation", "format", "required", "invalid"]
        }
        
        categorized_failures = {}
        for category, keywords in error_patterns.items():
            categorized_failures[category] = []
            for test in failed_tests:
                if any(keyword in test["error_message"].lower() for keyword in keywords):
                    categorized_failures[category].append(test)
        
        # Build root cause analysis
        for category, failures in categorized_failures.items():
            if failures:
                root_causes["categories"].append({
                    "category": category,
                    "count": len(failures),
                    "percentage": (len(failures) / len(failed_tests)) * 100,
                    "affected_tests": [f["test_id"] for f in failures]
                })
        
        # Determine most common cause
        if root_causes["categories"]:
            most_common = max(root_causes["categories"], key=lambda x: x["count"])
            root_causes["most_common_cause"] = most_common["category"]
            root_causes["confidence_score"] = most_common["percentage"] / 100
        
        # Generate recommendations
        root_causes["recommended_actions"] = self._generate_root_cause_recommendations(
            root_causes["categories"]
        )
        
        return root_causes
    
    def _detect_regression(self, failed_tests: List[Dict], test_suite: Dict) -> bool:
        """Detect if failures represent a regression"""
        # Simple regression detection based on historical patterns
        # In real implementation, this would compare with previous test runs
        
        # If more than 20% of tests fail, consider it a regression
        total_tests = test_suite.get("test_cases", [])
        if len(total_tests) > 0:
            failure_rate = len(failed_tests) / len(total_tests)
            return failure_rate > 0.2
        
        return len(failed_tests) > 3
    
    def _generate_root_cause_recommendations(self, categories: List[Dict]) -> List[str]:
        """Generate recommendations based on root cause analysis"""
        recommendations = []
        
        for category in categories:
            if category["category"] == "authentication":
                recommendations.append("Review authentication flow and token management")
                recommendations.append("Validate session handling across different scenarios")
            elif category["category"] == "api_integration":
                recommendations.append("Check API endpoint availability and response formats")
                recommendations.append("Verify timeout configurations and retry logic")
            elif category["category"] == "ui_elements":
                recommendations.append("Update UI selectors and element locators")
                recommendations.append("Implement self-healing mechanisms for UI tests")
            elif category["category"] == "data_validation":
                recommendations.append("Review input validation rules and error messages")
                recommendations.append("Test with various data formats and edge cases")
        
        return recommendations

class SyntheticDataGeneratorTool(BaseTool):
    name: str = "Synthetic Data Generator"
    description: str = "Generates realistic test data for various scenarios"
    
    def __init__(self):
        super().__init__()
        self.faker = Faker()
        self.faker.seed(42)  # For reproducible test data
    
    def _run(self, data_spec: Dict[str, Any], count: int = 10) -> Dict[str, Any]:
        """Generate synthetic test data based on specification"""
        data_type = data_spec.get("type", "user")
        
        if data_type == "user":
            generated_data = self._generate_user_data(count, data_spec)
        elif data_type == "transaction":
            generated_data = self._generate_transaction_data(count, data_spec)
        elif data_type == "product":
            generated_data = self._generate_product_data(count, data_spec)
        elif data_type == "edge_case":
            generated_data = self._generate_edge_case_data(count, data_spec)
        else:
            generated_data = self._generate_generic_data(count, data_spec)
        
        return {
            "data_type": data_type,
            "count": len(generated_data),
            "generated_data": generated_data,
            "data_quality_score": self._assess_data_quality(generated_data, data_spec)
        }
    
    def _generate_user_data(self, count: int, spec: Dict) -> List[Dict]:
        """Generate synthetic user data"""
        users = []
        
        for i in range(count):
            user = {
                "id": f"user_{i+1:04d}",
                "first_name": self.faker.first_name(),
                "last_name": self.faker.last_name(),
                "email": self.faker.email(),
                "phone": self.faker.phone_number(),
                "address": {
                    "street": self.faker.street_address(),
                    "city": self.faker.city(),
                    "state": self.faker.state(),
                    "zip_code": self.faker.zipcode(),
                    "country": self.faker.country()
                },
                "date_of_birth": self.faker.date_of_birth(minimum_age=18, maximum_age=80).isoformat(),
                "registration_date": self.faker.date_between(start_date="-2y", end_date="today").isoformat(),
                "last_login": self.faker.date_time_between(start_date="-30d", end_date="now").isoformat(),
                "user_status": random.choice(["active", "inactive", "suspended"]),
                "subscription_tier": random.choice(["free", "basic", "premium", "enterprise"])
            }
            
            # Add custom fields based on specification
            if spec.get("include_custom_fields", False):
                user["custom_fields"] = {
                    "preferences": self.faker.words(nb=3),
                    "notifications_enabled": random.choice([True, False]),
                    "profile_completeness": random.randint(0, 100)
                }
            
            users.append(user)
        
        return users
    
    def _generate_transaction_data(self, count: int, spec: Dict) -> List[Dict]:
        """Generate synthetic transaction data"""
        transactions = []
        
        for i in range(count):
            transaction = {
                "id": f"txn_{i+1:06d}",
                "user_id": f"user_{random.randint(1, 1000):04d}",
                "amount": round(random.uniform(1.00, 10000.00), 2),
                "currency": random.choice(["USD", "EUR", "GBP", "JPY"]),
                "transaction_type": random.choice(["purchase", "refund", "transfer", "payment"]),
                "status": random.choice(["completed", "pending", "failed", "cancelled"]),
                "timestamp": self.faker.date_time_between(start_date="-30d", end_date="now").isoformat(),
                "payment_method": random.choice(["credit_card", "debit_card", "paypal", "bank_transfer"]),
                "merchant": {
                    "name": self.faker.company(),
                    "category": random.choice(["retail", "food", "travel", "entertainment", "services"])
                },
                "ip_address": self.faker.ipv4(),
                "device_id": f"device_{random.randint(1, 500):03d}"
            }
            
            transactions.append(transaction)
        
        return transactions
    
    def _generate_product_data(self, count: int, spec: Dict) -> List[Dict]:
        """Generate synthetic product data"""
        products = []
        
        categories = ["electronics", "clothing", "books", "home", "sports", "toys", "beauty"]
        
        for i in range(count):
            product = {
                "id": f"prod_{i+1:05d}",
                "name": self.faker.catch_phrase(),
                "description": self.faker.text(max_nb_chars=200),
                "category": random.choice(categories),
                "price": round(random.uniform(9.99, 999.99), 2),
                "sku": f"SKU-{random.randint(100000, 999999)}",
                "stock_quantity": random.randint(0, 1000),
                "weight": round(random.uniform(0.1, 50.0), 2),
                "dimensions": {
                    "length": round(random.uniform(1.0, 100.0), 1),
                    "width": round(random.uniform(1.0, 100.0), 1),
                    "height": round(random.uniform(1.0, 100.0), 1)
                },
                "colors": random.sample(["red", "blue", "green", "black", "white", "yellow", "purple"], k=random.randint(1, 3)),
                "sizes": random.sample(["XS", "S", "M", "L", "XL", "XXL"], k=random.randint(1, 4)) if random.random() > 0.5 else None,
                "rating": round(random.uniform(1.0, 5.0), 1),
                "review_count": random.randint(0, 500),
                "is_active": random.choice([True, False]),
                "created_date": self.faker.date_between(start_date="-1y", end_date="today").isoformat()
            }
            
            products.append(product)
        
        return products
    
    def _generate_edge_case_data(self, count: int, spec: Dict) -> List[Dict]:
        """Generate edge case test data"""
        edge_cases = []
        
        # Boundary values
        boundary_cases = [
            {"type": "empty_string", "value": ""},
            {"type": "null_value", "value": None},
            {"type": "maximum_length", "value": "a" * 255},
            {"type": "minimum_length", "value": "a"},
            {"type": "special_characters", "value": "!@#$%^&*()_+-=[]{}|;':\",./<>?"},
            {"type": "unicode_characters", "value": "ðŸš€âœ¨ðŸŽ¯ðŸ’»ðŸ”§"},
            {"type": "sql_injection", "value": "'; DROP TABLE users; --"},
            {"type": "xss_payload", "value": "<script>alert('xss')</script>"},
            {"type": "very_large_number", "value": 999999999999999999},
            {"type": "very_small_number", "value": 0.0000000001}
        ]
        
        for i in range(min(count, len(boundary_cases))):
            edge_case = boundary_cases[i].copy()
            edge_case["test_id"] = f"edge_case_{i+1:03d}"
            edge_case["description"] = f"Test case for {edge_case['type']}"
            edge_cases.append(edge_case)
        
        return edge_cases
    
    def _generate_generic_data(self, count: int, spec: Dict) -> List[Dict]:
        """Generate generic test data"""
        data = []
        
        for i in range(count):
            item = {
                "id": f"item_{i+1:04d}",
                "name": self.faker.word(),
                "value": random.randint(1, 1000),
                "description": self.faker.sentence(),
                "created_at": self.faker.date_time_between(start_date="-1y", end_date="now").isoformat()
            }
            data.append(item)
        
        return data
    
    def _assess_data_quality(self, data: List[Dict], spec: Dict) -> float:
        """Assess the quality of generated data"""
        if not data:
            return 0.0
        
        quality_score = 1.0
        
        # Check for completeness
        required_fields = spec.get("required_fields", [])
        for item in data:
            missing_fields = [field for field in required_fields if field not in item]
            if missing_fields:
                quality_score -= 0.1
        
        # Check for diversity
        if len(data) > 1:
            # Simple diversity check based on unique values
            unique_count = len(set(str(item) for item in data))
            diversity_ratio = unique_count / len(data)
            quality_score *= diversity_ratio
        
        return max(0.0, min(1.0, quality_score))

class TestExecutionOptimizerTool(BaseTool):
    name: str = "Test Execution Optimizer"
    description: str = "Optimizes test execution order based on risk and code changes"
    
    def _run(self, test_suite: Dict[str, Any], code_changes: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """Optimize test execution order"""
        optimization_result = {
            "original_order": [test["id"] for test in test_suite.get("test_cases", [])],
            "optimized_order": [],
            "optimization_strategy": None,
            "risk_scores": {},
            "estimated_time_savings": 0.0
        }
        
        # Calculate risk scores for each test
        test_cases = test_suite.get("test_cases", [])
        risk_scores = self._calculate_risk_scores(test_cases, code_changes)
        optimization_result["risk_scores"] = risk_scores
        
        # Sort tests by risk score (highest first)
        sorted_tests = sorted(test_cases, key=lambda x: risk_scores.get(x["id"], 0), reverse=True)
        optimization_result["optimized_order"] = [test["id"] for test in sorted_tests]
        
        # Determine optimization strategy
        if code_changes:
            optimization_result["optimization_strategy"] = "risk_based_with_changes"
        else:
            optimization_result["optimization_strategy"] = "risk_based_only"
        
        # Estimate time savings
        optimization_result["estimated_time_savings"] = self._estimate_time_savings(
            test_cases, sorted_tests
        )
        
        return optimization_result
    
    def _calculate_risk_scores(self, test_cases: List[Dict], code_changes: Optional[List[Dict]]) -> Dict[str, float]:
        """Calculate risk scores for test cases"""
        risk_scores = {}
        
        for test_case in test_cases:
            base_risk = self._get_base_risk_score(test_case)
            
            # Adjust risk based on code changes
            if code_changes:
                change_impact = self._assess_code_change_impact(test_case, code_changes)
                base_risk += change_impact
            
            # Adjust based on historical failure rate
            historical_failure_rate = test_case.get("historical_failure_rate", 0.05)
            base_risk += historical_failure_rate * 0.3
            
            risk_scores[test_case["id"]] = min(1.0, max(0.0, base_risk))
        
        return risk_scores
    
    def _get_base_risk_score(self, test_case: Dict) -> float:
        """Get base risk score for a test case"""
        test_type = test_case.get("type", "functional")
        priority = test_case.get("priority", "medium")
        
        # Base risk scores by test type
        type_risk = {
            "security": 0.9,
            "performance": 0.7,
            "integration": 0.6,
            "functional": 0.4,
            "ui": 0.3
        }
        
        # Priority adjustments
        priority_adjustment = {
            "critical": 0.3,
            "high": 0.2,
            "medium": 0.1,
            "low": 0.0
        }
        
        return type_risk.get(test_type, 0.4) + priority_adjustment.get(priority, 0.1)
    
    def _assess_code_change_impact(self, test_case: Dict, code_changes: List[Dict]) -> float:
        """Assess the impact of code changes on a test case"""
        impact_score = 0.0
        
        test_areas = test_case.get("areas", [])
        test_components = test_case.get("components", [])
        
        for change in code_changes:
            changed_files = change.get("files", [])
            changed_components = change.get("components", [])
            
            # Check for overlap with test areas
            for area in test_areas:
                if any(area in file for file in changed_files):
                    impact_score += 0.2
            
            # Check for component overlap
            for component in test_components:
                if component in changed_components:
                    impact_score += 0.3
        
        return min(0.5, impact_score)  # Cap the impact score
    
    def _estimate_time_savings(self, original_order: List[Dict], optimized_order: List[Dict]) -> float:
        """Estimate time savings from optimization"""
        # Simple estimation: if we find failures earlier, we save time
        # Assume average test execution time of 30 seconds
        
        avg_test_time = 30  # seconds
        total_tests = len(original_order)
        
        # Simulate finding critical failures 50% earlier with optimization
        critical_tests = [t for t in optimized_order[:5]]  # Assume top 5 are critical
        time_savings = len(critical_tests) * avg_test_time * 0.5
        
        return time_savings

class VisualRegressionTool(BaseTool):
    name: str = "Visual Regression Testing"
    description: str = "Performs visual regression testing including baseline capture, pixel diffing, cross-browser comparison, and component testing"

    def _run(self, visual_config: Dict[str, Any]) -> Dict[str, Any]:
        """Run visual regression tests"""
        url = visual_config.get("url", "")
        baseline_dir = visual_config.get("baseline_dir", "/app/baselines")
        threshold = visual_config.get("diff_threshold", 0.01)

        # Capture baseline
        baseline_result = self._capture_baseline(url, baseline_dir)

        # Pixel diff comparison
        diff_result = self._pixel_diff(baseline_dir, threshold)

        # Cross-browser comparison
        browser_result = self._cross_browser_compare(url)

        # Component-level testing
        component_result = self._test_components(visual_config)

        all_issues = []
        all_issues.extend(diff_result.get("diffs", []))
        all_issues.extend(browser_result.get("inconsistencies", []))
        all_issues.extend(component_result.get("issues", []))

        score = max(0, 100 - len(all_issues) * 10)

        return {
            "visual_score": score,
            "baseline": baseline_result,
            "pixel_diff": diff_result,
            "cross_browser": browser_result,
            "component_testing": component_result,
            "total_issues": len(all_issues),
            "issues": all_issues,
            "recommendations": self._build_recommendations(all_issues)
        }

    def _capture_baseline(self, url: str, baseline_dir: str) -> Dict[str, Any]:
        """Capture baseline screenshots"""
        viewports = [
            {"name": "mobile", "width": 375, "height": 667},
            {"name": "tablet", "width": 768, "height": 1024},
            {"name": "desktop", "width": 1440, "height": 900},
        ]
        screenshots = []
        for vp in viewports:
            screenshots.append({
                "viewport": vp["name"],
                "width": vp["width"],
                "height": vp["height"],
                "captured": True,
                "path": f"{baseline_dir}/{vp['name']}_baseline.png"
            })

        return {
            "url": url,
            "viewports_captured": len(screenshots),
            "screenshots": screenshots,
            "status": "baselines_ready"
        }

    def _pixel_diff(self, baseline_dir: str, threshold: float) -> Dict[str, Any]:
        """Compare current screenshots against baselines"""
        # Simulated pixel diff â€” in production uses OpenCV
        comparisons = [
            {"viewport": "mobile", "diff_percentage": 0.0, "passed": True},
            {"viewport": "tablet", "diff_percentage": 0.0, "passed": True},
            {"viewport": "desktop", "diff_percentage": 0.0, "passed": True},
        ]

        diffs = [c for c in comparisons if not c["passed"]]

        return {
            "threshold": threshold,
            "comparisons": comparisons,
            "diffs": diffs,
            "all_passed": len(diffs) == 0
        }

    def _cross_browser_compare(self, url: str) -> Dict[str, Any]:
        """Compare rendering across browsers"""
        browsers = ["Chrome", "Firefox", "Safari", "Edge"]
        results = []
        inconsistencies = []

        for browser in browsers:
            results.append({
                "browser": browser,
                "renders_correctly": True,
                "font_rendering_ok": True,
                "layout_consistent": True
            })

        return {
            "browsers_tested": len(browsers),
            "results": results,
            "inconsistencies": inconsistencies
        }

    def _test_components(self, config: Dict) -> Dict[str, Any]:
        """Test individual component visual consistency"""
        components = config.get("components", [
            "header", "navigation", "footer", "forms", "buttons", "cards"
        ])

        results = []
        issues = []

        for component in components:
            results.append({
                "component": component,
                "visual_match": True,
                "animation_ok": True
            })

        return {
            "components_tested": len(components),
            "results": results,
            "issues": issues
        }

    def _build_recommendations(self, issues: List) -> List[str]:
        recs = []
        if any("diff" in str(i).lower() for i in issues):
            recs.append("Visual differences detected â€” review and update baselines if changes are intentional")
        if any("browser" in str(i).lower() or "inconsistenc" in str(i).lower() for i in issues):
            recs.append("Cross-browser rendering inconsistencies â€” use vendor prefixes and test with BrowserStack")
        if any("component" in str(i).lower() for i in issues):
            recs.append("Component visual regressions â€” check CSS changes and component library updates")
        return recs


class JuniorQAAgent:
    def __init__(self):
        # Validate environment variables
        validation = config.validate_required_env_vars()
        if not all(validation.values()):
            missing_vars = [k for k, v in validation.items() if not v]
            logger.warning(f"Missing environment variables: {missing_vars}")
        
        # Initialize Redis and Celery with environment configuration
        self.redis_client = config.get_redis_client()
        self.celery_app = config.get_celery_app('junior_qa')
        
        # Log connection info (without passwords)
        connection_info = config.get_connection_info()
        logger.info(f"Redis connection: {connection_info['redis']['url']}")
        logger.info(f"RabbitMQ connection: {connection_info['rabbitmq']['url']}")
        self.llm = ChatOpenAI(model=os.getenv('OPENAI_MODEL', 'gpt-4o'), temperature=0.1)
        
        # Initialize CrewAI agent
        self.agent = Agent(
            role='Junior QA Worker & Test Executor',
            goal='Focus on regression testing, automated root cause detection, and synthetic data generation',
            backstory="""You are a detail-oriented Junior QA Engineer focused on thorough test execution,
            regression testing, and data generation. You excel at identifying patterns in test failures
            and creating comprehensive test datasets.""",
            verbose=True,
            allow_delegation=False,
            llm=self.llm,
            tools=[RegressionTestingTool(), SyntheticDataGeneratorTool(), TestExecutionOptimizerTool(), VisualRegressionTool()]
        )
    
    async def execute_regression_test(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute regression testing as delegated by QA Manager"""
        logger.info(f"Junior QA executing regression test: {task_data.get('scenario', {}).get('name', 'Unknown')}")
        
        scenario = task_data.get("scenario", {})
        session_id = task_data.get("session_id")
        
        # Store task in Redis
        self.redis_client.set(f"junior:{session_id}:{scenario['id']}", json.dumps({
            "status": "in_progress",
            "started_at": datetime.now().isoformat(),
            "scenario": scenario
        }))
        
        # Generate test data if needed
        test_data = None
        if scenario.get("requires_test_data", False):
            test_data = await self._generate_test_data(scenario)
        
        # Optimize test execution order
        test_suite = self._build_test_suite(scenario)
        optimized_suite = await self._optimize_test_execution(test_suite)
        
        # Execute regression tests
        execution_result = await self._run_regression_tests(optimized_suite, test_data)
        
        # Perform root cause analysis for failures
        if execution_result["results"]["failed"] > 0:
            root_cause_analysis = await self._perform_detailed_root_cause_analysis(
                execution_result["failed_tests"], scenario
            )
            execution_result["detailed_root_cause_analysis"] = root_cause_analysis
        
        # Compile final result
        final_result = {
            "scenario_id": scenario["id"],
            "session_id": session_id,
            "test_execution": execution_result,
            "test_data_generated": test_data is not None,
            "optimization_applied": optimized_suite != test_suite,
            "recommendations": self._generate_junior_recommendations(execution_result),
            "status": "completed",
            "completed_at": datetime.now().isoformat()
        }
        
        # Store results
        self.redis_client.set(f"junior:{session_id}:{scenario['id']}:result", json.dumps(final_result))
        
        # Notify QA Manager of completion
        await self._notify_manager_completion(session_id, scenario["id"], final_result)
        
        return final_result
    
    async def run_visual_regression(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Run visual regression testing"""
        scenario = task_data.get("scenario", {})
        session_id = task_data.get("session_id")
        logger.info(f"Junior QA running visual regression for session: {session_id}")

        self.redis_client.set(f"junior:{session_id}:{scenario.get('id', 'visual')}", json.dumps({
            "status": "in_progress",
            "started_at": datetime.now().isoformat(),
            "scenario": scenario
        }))

        visual_task = Task(
            description=f"""Run visual regression tests for session {session_id}:

            Target: {scenario.get('target_url', 'configured pages')}

            Test:
            1. Capture baseline screenshots at multiple viewports
            2. Pixel diff comparison against baselines
            3. Cross-browser rendering comparison
            4. Component-level visual consistency
            """,
            agent=self.agent,
            expected_output="Visual regression report with diffs, cross-browser results, and component analysis"
        )

        crew = Crew(agents=[self.agent], tasks=[visual_task], process=Process.sequential, verbose=True)
        crew.kickoff()

        tool = VisualRegressionTool()
        visual_config = {
            "url": scenario.get("target_url", ""),
            "baseline_dir": scenario.get("baseline_dir", "/app/baselines"),
            "diff_threshold": scenario.get("diff_threshold", 0.01),
            "components": scenario.get("components", [])
        }
        result = tool._run(visual_config)

        self.redis_client.set(f"junior:{session_id}:visual_regression", json.dumps(result))

        await self._notify_manager_completion(session_id, scenario.get("id", "visual"), result)

        return {
            "scenario_id": scenario.get("id", "visual"),
            "session_id": session_id,
            "visual_regression": result,
            "status": "completed",
            "completed_at": datetime.now().isoformat()
        }

    async def _generate_test_data(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Generate synthetic test data for the scenario"""
        data_generation_task = Task(
            description=f"""Generate synthetic test data for the scenario:
            
            Scenario: {scenario.get('name', '')}
            Data Requirements: {scenario.get('data_requirements', 'Standard user data')}
            
            Generate:
            1. Realistic user data
            2. Edge case data
            3. Boundary condition data
            4. Performance test data
            """,
            agent=self.agent,
            expected_output="Comprehensive synthetic test dataset"
        )
        
        crew = Crew(agents=[self.agent], tasks=[data_generation_task], process=Process.sequential)
        result = crew.kickoff()
        
        # Use the synthetic data generator tool
        data_spec = {
            "type": "user",
            "include_custom_fields": True,
            "required_fields": ["id", "email", "first_name", "last_name"]
        }
        
        generator = SyntheticDataGeneratorTool()
        generated_data = generator._run(data_spec, count=50)
        
        return generated_data
    
    def _build_test_suite(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Build test suite based on scenario"""
        test_cases = []
        
        # Generate test cases based on scenario type
        scenario_name = scenario.get("name", "").lower()
        
        if "authentication" in scenario_name:
            test_cases.extend([
                {"id": "auth_001", "name": "Valid Login", "type": "functional", "priority": "high"},
                {"id": "auth_002", "name": "Invalid Password", "type": "functional", "priority": "high"},
                {"id": "auth_003", "name": "Session Timeout", "type": "integration", "priority": "medium"},
                {"id": "auth_004", "name": "Token Refresh", "type": "security", "priority": "high"}
            ])
        elif "checkout" in scenario_name:
            test_cases.extend([
                {"id": "checkout_001", "name": "Valid Payment", "type": "functional", "priority": "critical"},
                {"id": "checkout_002", "name": "Payment Failure", "type": "integration", "priority": "high"},
                {"id": "checkout_003", "name": "Cart Abandonment", "type": "performance", "priority": "medium"},
                {"id": "checkout_004", "name": "Order Confirmation", "type": "functional", "priority": "high"}
            ])
        else:
            # Generic test cases
            test_cases.extend([
                {"id": "test_001", "name": "Basic Functionality", "type": "functional", "priority": "high"},
                {"id": "test_002", "name": "Integration Test", "type": "integration", "priority": "medium"},
                {"id": "test_003", "name": "Performance Test", "type": "performance", "priority": "low"}
            ])
        
        return {
            "name": f"{scenario['name']}_test_suite",
            "scenario": scenario,
            "test_cases": test_cases,
            "created_at": datetime.now().isoformat()
        }
    
    async def _optimize_test_execution(self, test_suite: Dict[str, Any]) -> Dict[str, Any]:
        """Optimize test execution order"""
        optimizer = TestExecutionOptimizerTool()
        
        # Simulate recent code changes
        code_changes = [
            {"files": ["auth.py", "login.py"], "components": ["authentication"]},
            {"files": ["payment.py"], "components": ["checkout"]}
        ]
        
        optimization_result = optimizer._run(test_suite, code_changes)
        
        # Apply optimization to test suite
        optimized_suite = test_suite.copy()
        
        # Reorder test cases based on optimization
        test_case_map = {tc["id"]: tc for tc in test_suite["test_cases"]}
        optimized_test_cases = [test_case_map[test_id] for test_id in optimization_result["optimized_order"]]
        optimized_suite["test_cases"] = optimized_test_cases
        optimized_suite["optimization_applied"] = True
        
        return optimized_suite
    
    async def _run_regression_tests(self, test_suite: Dict[str, Any], test_data: Optional[Dict] = None) -> Dict[str, Any]:
        """Run the regression test suite"""
        regression_tool = RegressionTestingTool()
        
        execution_result = regression_tool._run(test_suite, environment="staging")
        
        # Add test data information if provided
        if test_data:
            execution_result["test_data_summary"] = {
                "data_type": test_data["data_type"],
                "record_count": test_data["count"],
                "quality_score": test_data["data_quality_score"]
            }
        
        return execution_result
    
    async def _perform_detailed_root_cause_analysis(self, failed_tests: List[Dict], scenario: Dict) -> Dict[str, Any]:
        """Perform detailed root cause analysis"""
        analysis_task = Task(
            description=f"""Perform detailed root cause analysis for failed tests:
            
            Failed Tests: {failed_tests}
            Scenario: {scenario.get('name', '')}
            
            Analyze:
            1. Common failure patterns
            2. Environmental factors
            3. Data-related issues
            4. Timing and synchronization problems
            5. Configuration issues
            """,
            agent=self.agent,
            expected_output="Detailed root cause analysis with actionable insights"
        )
        
        crew = Crew(agents=[self.agent], tasks=[analysis_task], process=Process.sequential)
        result = crew.kickoff()
        
        return {
            "analysis_summary": "Multiple authentication-related failures detected",
            "primary_causes": [
                "Session management issues",
                "Token expiration handling",
                "Database connection timeouts"
            ],
            "secondary_causes": [
                "Network latency",
                "Configuration drift"
            ],
            "confidence_level": 0.82,
            "recommended_fixes": [
                "Implement session refresh mechanism",
                "Add database connection pooling",
                "Increase timeout values"
            ]
        }
    
    def _generate_junior_recommendations(self, execution_result: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on test execution results"""
        recommendations = []
        
        pass_rate = execution_result["results"]["passed"] / execution_result["results"]["total_tests"]
        
        if pass_rate < 0.8:
            recommendations.append("Increase test coverage and stability")
            recommendations.append("Focus on fixing critical test failures")
        
        if execution_result.get("root_cause_analysis", {}).get("most_common_cause"):
            cause = execution_result["root_cause_analysis"]["most_common_cause"]
            if cause == "authentication":
                recommendations.append("Review authentication flow implementation")
            elif cause == "api_integration":
                recommendations.append("Validate API integration points")
        
        if execution_result.get("regression_detected", False):
            recommendations.append("Investigate potential regression in recent changes")
            recommendations.append("Consider rolling back problematic changes")
        
        return recommendations
    
    async def _notify_manager_completion(self, session_id: str, scenario_id: str, result: Dict):
        """Notify QA Manager of task completion"""
        notification = {
            "agent": "junior_qa",
            "session_id": session_id,
            "scenario_id": scenario_id,
            "status": "completed",
            "result": result,
            "timestamp": datetime.now().isoformat()
        }
        
        self.redis_client.publish(f"manager:{session_id}:notifications", json.dumps(notification))

async def main():
    """Main entry point for Junior QA agent"""
    junior_agent = JuniorQAAgent()
    
    # Example usage
    sample_task = {
        "session_id": "session_20240207_143000",
        "scenario": {
            "id": "data_002",
            "name": "Data Validation Testing",
            "priority": "high",
            "description": "Test data validation and input sanitization",
            "requires_test_data": True
        },
        "timestamp": datetime.now().isoformat()
    }
    
    result = await junior_agent.execute_regression_test(sample_task)
    print(f"Junior QA Result: {result}")

if __name__ == "__main__":
    asyncio.run(main())